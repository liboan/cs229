{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression! Predicting continuous value variable. E.g. self-driving car steering, predicting housing prices\n",
    "- Notation:\n",
    "    - Let $m$ denote # of training examples\n",
    "    - Let $n$ denote # of features\n",
    "    - Let $x$ denote \"input\" variables, or features\n",
    "    - Let $y$ denote \"output\" variables, or targets\n",
    "    - $(x,y)$ is one training example\n",
    "    - $i$-th training example will be $ (x^{(i)}, y^{(i)}) $\n",
    "- We feed our training set into a learning algorithm, which then outputs a function $h$, called the hypothesis.\n",
    "    - Hypothesis takes a feature value (e.g. living area) and outputs a target (e.g. estimated price)\n",
    "    - So, how to represent hypothesis?\n",
    "    - In this example (predicting housing prices, use linear representation $$ h(x) = \\theta_{0} + \\theta_{1}x$$\n",
    "- More generally, with regression problems (and others) we may well have more than one feature, so it'll look like:\n",
    "    - $$ h(x) = h_{\\theta}(x) = \\theta_{0} +  \\theta_{1}x_{1} + \\theta_{2}x_{2} $$\n",
    "    - We can express this more concisely by defining an $x_{0} = 1$...\n",
    "    - which gives us $$h_{\\theta} = \\sum_{i=0}^{n} \\theta_{i}x_{i} $$\n",
    "    - If we treat $\\theta$, $x$ as vectors then... $$ h_{\\theta}(x) = \\theta^Tx$$ (flipping theta to a row vector, dot-producting the two vectors together to get one scalar)\n",
    "- In our learning algo, $\\theta$s are parameters (for now, real #s), the learning algo needs to determine what their values should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to choose the correct $\\theta$s?\n",
    "    - We could have the learning algorithm make predictions that are accurate against the training set\n",
    "    - Or minimize the (halved) squared error b/w predicted and actual price...\n",
    "    $$ J(\\theta) =  \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)} )^2 $$\n",
    "- An algorithm: Gradient Descent\n",
    "    - Steps:\n",
    "        - Start with some $\\theta$ (say, $\\theta = 0$ )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
