{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression! Predicting continuous value variable. E.g. self-driving car steering, predicting housing prices\n",
    "- Notation:\n",
    "    - Let $m$ denote # of training examples\n",
    "    - Let $n$ denote # of features\n",
    "    - Let $x$ denote \"input\" variables, or features\n",
    "    - Let $y$ denote \"output\" variables, or targets\n",
    "    - $(x,y)$ is one training example\n",
    "    - $i$-th training example will be $ (x^{(i)}, y^{(i)}) $\n",
    "- We feed our training set into a learning algorithm, which then outputs a function $h$, called the hypothesis.\n",
    "    - Hypothesis takes a feature value (e.g. living area) and outputs a target (e.g. estimated price)\n",
    "    - So, how to represent hypothesis?\n",
    "    - In this example (predicting housing prices, use linear representation $$ h(x) = \\theta_{0} + \\theta_{1}x$$\n",
    "- More generally, with regression problems (and others) we may well have more than one feature, so it'll look like:\n",
    "    $$ h(x) = h_{\\theta}(x) = \\theta_{0} +  \\theta_{1}x_{1} + \\theta_{2}x_{2} $$\n",
    "    - We can express this more concisely by defining an $x_{0} = 1$...\n",
    "    - which gives us $$h_{\\theta} = \\sum_{i=0}^{n} \\theta_{i}x_{i} $$\n",
    "    - If we treat $\\theta$, $x$ as vectors then... $$ h_{\\theta}(x) = \\theta^Tx$$ (flipping theta to a row vector, dot-producting the two vectors together to get one scalar)\n",
    "- In our learning algo, $\\theta$s are parameters (for now, real #s), the learning algo needs to determine what their values should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to choose the correct $\\theta$s?\n",
    "    - We could have the learning algorithm make predictions that are accurate against the training set\n",
    "    - Or minimize the (halved) squared error b/w predicted and actual price...\n",
    "    $$ J(\\theta) =  \\frac{1}{2} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)} )^2 $$\n",
    "- An algorithm: Gradient Descent\n",
    "    - Steps:\n",
    "        - Start with some $\\theta$ (say, $\\theta = \\vec{0}$ or some other vector of values)\n",
    "        - Keep changing $\\theta$ to minimize $J(\\theta)$\n",
    "    - Consider a 3D landscape with two axes being $\\theta$ values, and the z-axis (height) as $J(\\theta)$. Goal is to \"step downhill\" in the direction with steepest change in order to reach the minimum \"height\" (local minimum of $J(\\theta)$)\n",
    "    - Gradient Descent may land you in a different local minimum depending on starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The updating process of gradient descent can be expressed as: $$ \\theta_{i} := \\theta_{i} - \\alpha\\frac{\\partial}{\\partial\\theta_{i}} J(\\theta)  $$\n",
    "    - $ := $ means \"set equal to\"\n",
    "    - Applying this to our housing price problem (in the case of one training example)\n",
    "        - Substitute for $J(\\theta)$, chain rule the partial derivative, substitute for $h_{\\theta}(x)$, cancel all $\\theta_{k}x_{k}$ pairs except for the $i$-th. Result: $$ \\theta_{i} := \\theta_{i} - \\alpha(h_{\\theta}(x)-y)x_{i} $$ \n",
    "    - In the algo, $\\alpha$ is a parameter \"learning rate\". In gradient descent, once you've chosen which direction to step in, it determines how large a step you make.\n",
    "    - $\\alpha$ is often hand-set and needs to be picked carefully- might be too slow and take a while, or too fast and overshoot the minima\n",
    "    - For $m$ training examples, algo becomes: \n",
    "    $$ \\theta_{i} := \\theta_{i} - \\alpha \\sum_{j=1}^{m} (h_{\\theta}(x^{(j)})-y^{(j)})x_{i}^{(j)} $$\n",
    "    - Question: Where does the gradient come in? Work backwards from above, I think it looks like this...\n",
    "    $$ \\theta_{i} := \\theta_{i} - \\alpha \\left(\\frac{\\partial}{\\partial\\theta_{i}} J(\\theta)\\right) $$\n",
    "        - We are computing this partial for each $\\theta_{i}$ in the vector $\\vec\\theta$. Basically we are computing the components of the new $\\vec\\theta$. \n",
    "        - Recall the definition of a gradient: $$ \\nabla f(x_1, x_2, ..., x_n) = \\big\\langle \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n}, \\big\\rangle $$\n",
    "        - So we're basically setting our $\\vec\\theta$ vector based on the gradient of $J(\\theta)$, which is a function of the $\\vec\\theta$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent in action in Ordinary Least Squares Regression\n",
    "    - Thankfully, $j(\\theta)$ in this case is a quadratic, bowl-shaped function with one minimum \n",
    "    - As the learning algo approaches the single minimum, the steps decrease in size even though there's a fixed $\\alpha$, this is because $J(\\theta)$ (the other part) is an error function and the error is decreasing\n",
    "- We call this algo **Batch Gradient Descent**!\n",
    "    - \"Batch\" as in in every step, for every $\\theta_i$ we check (and sum) our entire set of $m$ training examples.\n",
    "    - However, it is computationally expensive for a large $m$- summing from one to $m$ for every $\\theta_i$ just to take *one step*!\n",
    "- Alternative for large training sets: **Stochastic Gradient Descent**!\n",
    "    - Steps:\n",
    "        - Set some $\\theta$, and repeat until convergence the following:\n",
    "            - For $j =1$ to $j = m$: Perform a gradient descent update using *only* the $j$-th training example, that is- $$ \\theta_i := \\theta_i - \\alpha\\left(h_{\\theta}(x^{(j)}) - y^{(j)}\\right)x^{(j)}\n",
    "                $$\n",
    "            - ...for every $\\theta_i$ \n",
    "    - So in every update, you iterate over the training set first, and in every iteration you pull out *one* training example and update.\n",
    "    - **My understanding**: \n",
    "        - Superficially, it appears as though I'm still performing $m \\times n$ operations...\n",
    "        - But I'm updating much more frequently, which allows me to converge faster. I update every feature per one training example \n",
    "47:40\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "0.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
